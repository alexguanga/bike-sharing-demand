{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Fields\n",
    "- datetime - hourly date + timestamp  \n",
    "- season \n",
    "    -  1 = spring, 2 = summer, 3 = fall, 4 = winter \n",
    "- holiday - whether the day is considered a holiday\n",
    "- workingday - whether the day is neither a weekend nor holiday\n",
    "- weather \n",
    "    - 1: Clear, Few clouds, Partly cloudy, Partly cloudy \n",
    "    - 2: Mist + Cloudy, Mist + Broken clouds, Mist + Few clouds, Mist \n",
    "    - 3: Light Snow, Light Rain + Thunderstorm + Scattered clouds, Light Rain + Scattered clouds \n",
    "    - 4: Heavy Rain + Ice Pallets + Thunderstorm + Mist, Snow + Fog \n",
    "- temp - temperature in Celsius\n",
    "- atemp - \"feels like\" temperature in Celsius\n",
    "- humidity - relative humidity\n",
    "- windspeed - wind speed\n",
    "- casual - number of non-registered user rentals initiated\n",
    "- registered - number of registered user rentals initiated\n",
    "- count - number of total rentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning:\n",
    "1. A datevalue might not always be in the correct format. Make sure that you have updated to the correct dtype.\n",
    "\n",
    "\n",
    "2. The `extract_dateinfo` is a great function created by fast.ai. This function created multiple layers of different segmentation using the date.\n",
    "\n",
    "\n",
    "3. Remember that a normal distribution is not always the best distribution to use (talking about the predictor variable: Count, Registered, & Casual)\n",
    "\n",
    "\n",
    "4. A logistic transformation is sometimes preferred. When dealing with highly skewed data, our model might have difficulties correctly making prediction across the possible target variables. Hence, if log transform the highly skewed variable, we might find that distribution is more normal allowing our model to have a similar methodology (and thus, similar residuals) across the possible target variable.\n",
    "    - If we train a model on a highly skew data, we often tend to see residuals have a different distribution across different ranges of our target variable.\n",
    "\n",
    "\n",
    "5. Use the `correlation` function located in the \"/Users/alexguanga/All_Projects/ds-portfolio/DataScience_Code/correlation_abs.py\" to find the plot the correlation.\n",
    "\n",
    "\n",
    "6. Remember that we need to create your own scorer function, you need to specify the function to sklearn library `make_scorer`\n",
    "\n",
    "```python\n",
    "def scorer(actual, predicted):\n",
    "    sle = (np.power(np.log(np.array((actual))+1) - \n",
    "            np.log(np.array(np.abs(predicted))+1), 2))\n",
    "    msle = np.mean(sle)\n",
    "    return (np.sqrt(msle))\n",
    "# Creating the scorer function\n",
    "rmse_scorer = make_scorer(scorer, greater_is_better=False)\n",
    "\n",
    "# 10 fold cross validation\n",
    "cv_score = cross_val_score(rf_model, train_on_dum, train_cnt_labels_dum, cv=10, scoring=rmse_scorer)\n",
    "```\n",
    "7. When you're transforming multiple columns through the label encoders (this is useful for models that do not put assign value on the magntitude like regression), we have to create a dictionary to encode the label.\n",
    "\n",
    "```python\n",
    "from sklearn import preprocessing\n",
    "from collections import defaultdict\n",
    "\n",
    "d = defaultdict(preprocessing.LabelEncoder)\n",
    "data[encoded_cols] = data[encoded_cols].apply(lambda x: d[x.name].fit_transform(x))\n",
    "```\n",
    "\n",
    "8. Hyperparamter Tuning:\n",
    "    - https://www.kaggle.com/willkoehrsen/intro-to-model-tuning-grid-and-random-search\n",
    "    - https://towardsdatascience.com/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n",
    "    - https://www.analyticsvidhya.com/blog/2016/03/complete-guide-parameter-tuning-xgboost-with-codes-python/\n",
    "    - https://machinelearningmastery.com/avoid-overfitting-by-early-stopping-with-xgboost-in-python/\n",
    "    - **Current hyperparameters**: https://medium.com/@mateini_12893/doing-xgboost-hyper-parameter-tuning-the-smart-way-part-1-of-2-f6d255a45dde\n",
    "    \n",
    "    \n",
    "9. Cycling Variables:\n",
    "    - By pure chance, I came upon an article that explains certain variables are cyclical. For example, if we look at hours or days, our model won't be able to understand that the hour 0 and 24 are close to one another.\n",
    "    - http://blog.davidkaleko.com/feature-engineering-cyclical-features.html\n",
    "    \n",
    "    \n",
    "10. `Interpolate`\n",
    "    - Interpolates gives you the flexibility to fill the missing values with many kinds of interpolations between the values like linear (which fillna does not provide).\n",
    "    \n",
    "11. If you would like to plot multiple graphs in the same view, you must do...\n",
    "    ```python\n",
    "    # Dropping some variables from the graph\n",
    "    figs, ax = plt.subplots(3, 1, figsize=(20, 12), sharex=True)\n",
    "    val_1.drop(PREDICTORS).plot.bar(color='b', ax=axs[0], title=\"Val 1\");\n",
    "    val_2.drop(PREDICTORS).plot.bar(color='b', ax=axs[1], title=\"Val 2\");\n",
    "    val_3.drop(PREDICTORS).plot.bar(color='b', ax=axs[2], title=\"Val 3\");\n",
    "    ```\n",
    "    \n",
    "12. If you like to group the variables by their datatype, use...\n",
    "    ```python\n",
    "    data.columns.to_series().groupby(data.dtypes).groups\n",
    "    ```\n",
    "\n",
    "13. `train_test_split` returns good results. Hence, while I CV are a great methodolgies, `train_test_split` in our example, show improved result.\n",
    "\n",
    "\n",
    "14. To remove warning that should not be warning (after some research on Stackerflow, you'll realize errors that should not be error), use the following:\n",
    "    - ```python \n",
    "        np.warnings.filterwarnings('ignore')```\n",
    "\n",
    "15. The `run_model` is a function created by me that will return two models: one trained via all the variables and another trained via the top variables! \n",
    "    - Also, parameters are passed into the function!\n",
    "    - Path `\"Users/alexguanga/All_Projects/ds-portfolio/DataScience_Code/run_model.py\"`\n",
    "    - A few other function that are useful are in `\"Users/alexguanga/All_Projects/ds-portfolio/DataScience_Code/modeling_functions.py\"`\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "- Performance was the strongest for the XGB Regressor model. However, the Gradient Boosting Regressor was also pretty strong!\n",
    "- Random Forest Regressor was suprsingly bad... when trained with the cross validation, it was suboptimal relative to XGB Regressor or Gradient Boosting Regressor.\n",
    "- Another intersting feature in this dataset was that it had 3 target variables, where two variables were the summation of the main target variables.\n",
    "    - When trained indepdently (the two target that were the summation of the main one), the performance varies but overall, the model performed best when trained on the main target variable.\n",
    "- The best methodology used in the project was combining models.. I combined the XGBoost Regressor and the Gradient Boosting Regressor.\n",
    "    - There were 4 models that I averaged to calculate my final predictions.\n",
    "        1. Gradient Boosting Model on the main target variable using all the variables.\n",
    "        2. Gradient Boosting Model on the main target variable using the top variables.\n",
    "        3. XGBoosting Regressor model on the main target variable using all the variables.\n",
    "        4. XGBoosting Regressor model on the main target variable using the top variables.\n",
    "-  **Final RMLSE was 0.41143 (11th percentile)**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
